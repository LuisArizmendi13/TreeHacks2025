{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e5dba6-7f9d-4555-8854-3b4a7d910479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_vars import ROBO, MY_KEY\n",
    "VIDEO1 = \"/home/ubuntu/TreeHacks2025/data/videos/videos/00049.mp4\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37091107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: micromamba\n",
      "Requirement already satisfied: dataclasses-json in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (0.6.7)\n",
      "Requirement already satisfied: supervision in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (0.25.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from dataclasses-json) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from dataclasses-json) (0.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.7 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (1.3.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.5.5.64 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (4.11.0)\n",
      "Requirement already satisfied: pillow>=9.4 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (10.4.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (2.32.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from supervision) (4.67.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json) (24.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from requests>=2.26.0->supervision) (2025.1.31)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/miniconda/envs/treehack/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.17.0)\n",
      "✅ Everything installed correctly!\n"
     ]
    }
   ],
   "source": [
    "#!pip install deep-sort-realtime \n",
    "# pip install transformers torch torchvision openai \n",
    "#!pip install ultralytics opencv-python torch torchvision torchaudio easyocr numpy\n",
    "#!pip install ultralytics opencv-python pillow \n",
    "#!pip install ultralytics opencv-python pandas tqdm \n",
    "# Install required libraries if not already installed\n",
    "#!pip install ultralytics torch torchvision\n",
    "# Fix missing dependencies for inference-sdk\n",
    "\n",
    "\n",
    "# Install all required dependencies in one go\n",
    "!micromamba install -n treehack inference-sdk opencv numpy requests aiohttp dataclasses-json supervision -c conda-forge -y\n",
    "!pip install dataclasses-json supervision\n",
    "\n",
    "# Verify installation\n",
    "import inference_sdk\n",
    "import cv2\n",
    "import dataclasses_json\n",
    "import supervision\n",
    "\n",
    "print(\"✅ Everything installed correctly!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1cf45f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'frame': 0, 'label': 'pedestrianCrossing', 'confidence': 0.8319253325462341, 'bbox': {'x': 2060, 'y': 788, 'width': 75, 'height': 67}}, {'frame': 0, 'label': 'pedestrianCrossing', 'confidence': 0.7661900520324707, 'bbox': {'x': 1826, 'y': 701, 'width': 176, 'height': 160}}], [{'frame': 179, 'label': 'pedestrianCrossing', 'confidence': 0.8860577344894409, 'bbox': {'x': 2389, 'y': 527, 'width': 347, 'height': 328}}]]\n",
      "✅ Inference complete! Detection results returned.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "\n",
    "# Roboflow API Setup\n",
    "API_KEY = ROBO  # Load API Key securely\n",
    "MODEL_ID = \"lisa-bjgh5/2\"  # Your YOLOv8 LISA Model ID\n",
    "CLIENT = InferenceHTTPClient(api_url=\"https://detect.roboflow.com\", api_key=API_KEY)\n",
    "\n",
    "def detect_objects_in_frame(frame, frame_idx):\n",
    "    \"\"\"Detect objects in a single frame using the API.\"\"\"\n",
    "    frame_path = f\"temp_frame_{frame_idx}.jpg\"\n",
    "    cv2.imwrite(frame_path, frame)\n",
    "\n",
    "    try:\n",
    "        result = CLIENT.infer(frame_path, model_id=MODEL_ID)\n",
    "        frame_detections = []\n",
    "\n",
    "        for pred in result.get(\"predictions\", []):\n",
    "            frame_detections.append({\n",
    "                \"frame\": frame_idx,\n",
    "                \"label\": pred[\"class\"],\n",
    "                \"confidence\": pred[\"confidence\"],\n",
    "                \"bbox\": {\n",
    "                    \"x\": int(pred[\"x\"]),\n",
    "                    \"y\": int(pred[\"y\"]),\n",
    "                    \"width\": int(pred[\"width\"]),\n",
    "                    \"height\": int(pred[\"height\"])\n",
    "                }\n",
    "            })\n",
    "\n",
    "        os.remove(frame_path)  # Clean up temp frame\n",
    "        return frame_detections\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "        os.remove(frame_path)  # Ensure cleanup\n",
    "        return []\n",
    "\n",
    "def process_video_frames(video_path):\n",
    "    \"\"\"Extracts three key frames from a video and runs object detection.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = [0, total_frames // 2, total_frames - 1]  # Beginning, middle, end\n",
    "    detections = []\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error: Could not read frame {frame_idx}\")\n",
    "            continue\n",
    "\n",
    "        frame_detections = detect_objects_in_frame(frame, frame_idx)\n",
    "        if frame_detections:\n",
    "            detections.append(frame_detections)\n",
    "\n",
    "        time.sleep(0.2)  # Avoid API rate limits\n",
    "\n",
    "    cap.release()\n",
    "    return detections\n",
    "\n",
    "# Example usage\n",
    "detections = process_video_frames(VIDEO1)\n",
    "print(detections)\n",
    "\n",
    "print(\"✅ Inference complete! Detection results returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc929dd9",
   "metadata": {},
   "source": [
    "How It Works\n",
    "1. Extracts 8 frames from a 5-second video.\n",
    "2. Encodes frames using BLIP-2’s Vision Transformer (ViT).\n",
    "3. Aggregates frame embeddings (mean pooling).\n",
    "4. Sends embeddings + question to ChatGPT.\n",
    "5. ChatGPT generates an answer based on the video context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b719d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "client = openai.OpenAI(api_key=MY_KEY) \n",
    "\n",
    "def generate_chatgpt_response(detected_objects):\n",
    "    \"\"\"\n",
    "    Uses detected objects as context and queries ChatGPT-4o for a video description.\n",
    "    \"\"\"\n",
    "    object_summary = \", \".join([f\"{count} {obj}\" for obj, count in detected_objects.items()])\n",
    "    prompt = f\"The video contains the following objects: {object_summary}. Describe what is happening in the video.\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI that describes video scenes based on detected objects.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d4ca91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 2 cars, 3.4ms\n",
      "Speed: 2.0ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 cars, 3.7ms\n",
      "Speed: 1.6ms preprocess, 3.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 cars, 1 traffic light, 3.6ms\n",
      "Speed: 1.5ms preprocess, 3.6ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 car, 3.7ms\n",
      "Speed: 1.7ms preprocess, 3.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 1 car, 3.7ms\n",
      "Speed: 1.6ms preprocess, 3.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 cars, 3.7ms\n",
      "Speed: 1.7ms preprocess, 3.7ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 cars, 3.9ms\n",
      "Speed: 2.2ms preprocess, 3.9ms inference, 0.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 416x640 2 cars, 3.6ms\n",
      "Speed: 1.5ms preprocess, 3.6ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Frame 25: Unknown | Cars -> Left: {'Close': 0, 'Far': 0}, Right: {'Close': 0, 'Far': 1}, Front: {'Close': 1, 'Far': 0}\n",
      "Frame 51: Unknown | Cars -> Left: {'Close': 0, 'Far': 0}, Right: {'Close': 1, 'Far': 1}, Front: {'Close': 0, 'Far': 0}\n",
      "Frame 76: Unknown | Cars -> Left: {'Close': 0, 'Far': 0}, Right: {'Close': 1, 'Far': 0}, Front: {'Close': 0, 'Far': 0}\n",
      "Frame 102: Unknown | Cars -> Left: {'Close': 0, 'Far': 0}, Right: {'Close': 1, 'Far': 0}, Front: {'Close': 0, 'Far': 0}\n",
      "Frame 127: Unknown | Cars -> Left: {'Close': 0, 'Far': 0}, Right: {'Close': 1, 'Far': 1}, Front: {'Close': 0, 'Far': 0}\n",
      "Frame 153: Unknown | Cars -> Left: {'Close': 0, 'Far': 0}, Right: {'Close': 1, 'Far': 1}, Front: {'Close': 0, 'Far': 0}\n",
      "Frame 179: Unknown | Cars -> Left: {'Close': 0, 'Far': 0}, Right: {'Close': 0, 'Far': 1}, Front: {'Close': 1, 'Far': 0}\n",
      "✅ Detection complete! Results returned.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")  # Use \"yolov8m.pt\" for better accuracy\n",
    "\n",
    "# Thresholds\n",
    "MIN_SHIFT_THRESHOLD = 20  # Pixels for left/right movement\n",
    "FORWARD_THRESHOLD = 1.1   # 10% area increase for forward movement\n",
    "UNKNOWN_THRESHOLD = 10    # Threshold for \"Unknown\" detection\n",
    "\n",
    "def detect_cars_in_frame(frame, frame_num, frame_center):\n",
    "    \"\"\"Detect cars and return their positions.\"\"\"\n",
    "    results = model(frame)\n",
    "    car_positions = {\"left\": [], \"right\": [], \"front\": []}\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            label = result.names[int(box.cls[0])]\n",
    "\n",
    "            if label == \"car\":\n",
    "                box_center_x = (x1 + x2) // 2\n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                area = width * height  # Box area for distance estimation\n",
    "\n",
    "                # Categorize position\n",
    "                if box_center_x < frame_center * 0.75:\n",
    "                    car_positions[\"left\"].append((box_center_x, area))\n",
    "                elif box_center_x > frame_center * 1.25:\n",
    "                    car_positions[\"right\"].append((box_center_x, area))\n",
    "                else:\n",
    "                    car_positions[\"front\"].append((box_center_x, area))\n",
    "\n",
    "    return car_positions\n",
    "\n",
    "def analyze_camera_movement(prev_data, curr_data):\n",
    "    \"\"\"Analyze camera movement based on object positions.\"\"\"\n",
    "    if not prev_data or not curr_data:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    movement = \"Unknown\"\n",
    "\n",
    "    # Get average X positions\n",
    "    avg_prev_left = np.mean([x for x, _ in prev_data[\"left\"]]) if prev_data[\"left\"] else None\n",
    "    avg_curr_left = np.mean([x for x, _ in curr_data[\"left\"]]) if curr_data[\"left\"] else None\n",
    "\n",
    "    avg_prev_right = np.mean([x for x, _ in prev_data[\"right\"]]) if prev_data[\"right\"] else None\n",
    "    avg_curr_right = np.mean([x for x, _ in curr_data[\"right\"]]) if curr_data[\"right\"] else None\n",
    "\n",
    "    avg_prev_center = np.mean([x for x, _ in prev_data[\"front\"]]) if prev_data[\"front\"] else None\n",
    "    avg_curr_center = np.mean([x for x, _ in curr_data[\"front\"]]) if curr_data[\"front\"] else None\n",
    "\n",
    "    # Detect left/right movement\n",
    "    if avg_prev_left and avg_curr_left and avg_curr_left > avg_prev_left + MIN_SHIFT_THRESHOLD:\n",
    "        movement = \"Moving Right\"\n",
    "    elif avg_prev_right and avg_curr_right and avg_curr_right < avg_prev_right - MIN_SHIFT_THRESHOLD:\n",
    "        movement = \"Moving Left\"\n",
    "\n",
    "    # Detect forward movement\n",
    "    prev_avg_area = np.mean([area for _, area in prev_data[\"front\"]]) if prev_data[\"front\"] else None\n",
    "    curr_avg_area = np.mean([area for _, area in curr_data[\"front\"]]) if curr_data[\"front\"] else None\n",
    "\n",
    "    if prev_avg_area and curr_avg_area:\n",
    "        if curr_avg_area > prev_avg_area * FORWARD_THRESHOLD:\n",
    "            movement = \"Moving Forward\"\n",
    "\n",
    "    # Strengthen \"Unknown\" detection\n",
    "    if movement in [\"Moving Left\", \"Moving Right\", \"Moving Forward\"]:\n",
    "        if avg_curr_center is None or avg_prev_center is None or abs(avg_curr_center - avg_prev_center) < UNKNOWN_THRESHOLD:\n",
    "            movement = \"Unknown\"\n",
    "\n",
    "    # Distance estimation\n",
    "    car_summary = {\n",
    "        \"Left\": {\"Close\": sum(1 for _, area in curr_data[\"left\"] if area > 50000),\n",
    "                 \"Far\": sum(1 for _, area in curr_data[\"left\"] if area <= 50000)},\n",
    "        \"Right\": {\"Close\": sum(1 for _, area in curr_data[\"right\"] if area > 50000),\n",
    "                  \"Far\": sum(1 for _, area in curr_data[\"right\"] if area <= 50000)},\n",
    "        \"Front\": {\"Close\": sum(1 for _, area in curr_data[\"front\"] if area > 50000),\n",
    "                  \"Far\": sum(1 for _, area in curr_data[\"front\"] if area <= 50000)}\n",
    "    }\n",
    "\n",
    "    return movement, car_summary\n",
    "\n",
    "def process_video_frames(video_path):\n",
    "    \"\"\"Extracts 8 frames from a video and analyzes movement.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_center = frame_width // 2\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, 8).astype(int).tolist()\n",
    "    prev_frame_data = None\n",
    "    movement_summary = []\n",
    "\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error: Could not read frame {idx}\")\n",
    "            continue\n",
    "\n",
    "        curr_frame_data = detect_cars_in_frame(frame, idx, frame_center)\n",
    "\n",
    "        if prev_frame_data is not None:\n",
    "            movement, car_info = analyze_camera_movement(prev_frame_data, curr_frame_data)\n",
    "            movement_summary.append((idx, movement, car_info))\n",
    "\n",
    "        prev_frame_data = curr_frame_data\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Print movement analysis\n",
    "    for frame, movement, cars in movement_summary:\n",
    "        print(f\"Frame {frame}: {movement} | Cars -> Left: {cars['Left']}, Right: {cars['Right']}, Front: {cars['Front']}\")\n",
    "\n",
    "    return movement_summary\n",
    "\n",
    "# Example usage\n",
    "movement_summary = process_video_frames(VIDEO1)\n",
    "print(\"✅ Detection complete! Results returned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df11df0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2790442485.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 19\u001b[0;36m\u001b[0m\n\u001b[0;31m    Detects traffic signs and extracts text from them.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# FOR SHOWING THE IMAGES: \n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"yolov8n.pt\")  # You can use \"yolov8m.pt\" for better accuracy\n",
    "\n",
    "# Load video\n",
    "video_path = VIDEO1  # Use predefined VIDEO1 variable\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"processed_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get video properties\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_center = frame_width // 2  # Middle of the frame\n",
    "\n",
    "# Select 8 evenly spaced frames\n",
    "frame_indices = np.linspace(0, total_frames - 1, 8).astype(int).tolist()\n",
    "\n",
    "# Store detected object positions\n",
    "object_positions = {}\n",
    "prev_frame_data = None  # Store previous frame's detection data\n",
    "\n",
    "# Thresholds\n",
    "MIN_SHIFT_THRESHOLD = 20  # Pixels required for detecting left/right movement\n",
    "FORWARD_THRESHOLD = 1.1   # 10% area increase for forward movement\n",
    "UNKNOWN_THRESHOLD = 10    # Strengthened \"Unknown\" detection\n",
    "\n",
    "def detect_cars(frame, frame_num):\n",
    "    \"\"\"Detect cars and track their positions.\"\"\"\n",
    "    results = model(frame)\n",
    "    car_positions = {\"left\": [], \"right\": [], \"front\": []}\n",
    "    total_x_positions = []  # To estimate left/right movement\n",
    "    total_areas = []  # To estimate forward movement\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            label = result.names[int(box.cls[0])]\n",
    "            box_center_x = (x1 + x2) // 2\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            area = width * height  # Box area for motion detection\n",
    "\n",
    "            if label == \"car\":  # Only process car detections\n",
    "                total_x_positions.append(box_center_x)\n",
    "                total_areas.append(area)\n",
    "\n",
    "                # Categorize cars based on position in the frame\n",
    "                if box_center_x < frame_center * 0.75:\n",
    "                    position = \"Left\"\n",
    "                    car_positions[\"left\"].append((box_center_x, area))\n",
    "                elif box_center_x > frame_center * 1.25:\n",
    "                    position = \"Right\"\n",
    "                    car_positions[\"right\"].append((box_center_x, area))\n",
    "                else:\n",
    "                    position = \"Front\"\n",
    "                    car_positions[\"front\"].append((box_center_x, area))\n",
    "\n",
    "                # Estimate distance (Close or Far)\n",
    "                distance = \"Close\" if area > 50000 else \"Far\"\n",
    "\n",
    "                # Draw bounding box and label\n",
    "                color = (0, 0, 255) if distance == \"Close\" else (255, 0, 0)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, f\"Car ({position}, {distance})\", \n",
    "                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "    # Store detected objects for motion analysis\n",
    "    object_positions[frame_num] = car_positions\n",
    "\n",
    "    # Save processed frame\n",
    "    output_path = os.path.join(output_dir, f\"frame_{frame_num}.jpg\")\n",
    "    cv2.imwrite(output_path, frame)\n",
    "\n",
    "    # Display frame\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Frame {frame_num}\")\n",
    "    plt.show()\n",
    "\n",
    "    return car_positions\n",
    "\n",
    "def analyze_camera_movement(prev_data, curr_data):\n",
    "    \"\"\"Analyze camera movement using only the previous frame.\"\"\"\n",
    "    if prev_data is None or curr_data is None:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    movement = \"Unknown\"\n",
    "\n",
    "    # Get average X positions of objects in each region\n",
    "    avg_prev_left = np.mean([x for x, _ in prev_data[\"left\"]]) if prev_data[\"left\"] else None\n",
    "    avg_curr_left = np.mean([x for x, _ in curr_data[\"left\"]]) if curr_data[\"left\"] else None\n",
    "\n",
    "    avg_prev_right = np.mean([x for x, _ in prev_data[\"right\"]]) if prev_data[\"right\"] else None\n",
    "    avg_curr_right = np.mean([x for x, _ in curr_data[\"right\"]]) if curr_data[\"right\"] else None\n",
    "\n",
    "    avg_prev_center = np.mean([x for x, _ in prev_data[\"front\"]]) if prev_data[\"front\"] else None\n",
    "    avg_curr_center = np.mean([x for x, _ in curr_data[\"front\"]]) if curr_data[\"front\"] else None\n",
    "\n",
    "    # Left/right movement detection\n",
    "    if avg_prev_left and avg_curr_left and avg_curr_left > avg_prev_left + MIN_SHIFT_THRESHOLD:\n",
    "        movement = \"Moving Right\"\n",
    "    elif avg_prev_right and avg_curr_right and avg_curr_right < avg_prev_right - MIN_SHIFT_THRESHOLD:\n",
    "        movement = \"Moving Left\"\n",
    "\n",
    "    # Forward detection (More lenient)\n",
    "    prev_avg_area = np.mean([area for _, area in prev_data[\"front\"]]) if prev_data[\"front\"] else None\n",
    "    curr_avg_area = np.mean([area for _, area in curr_data[\"front\"]]) if curr_data[\"front\"] else None\n",
    "\n",
    "    if prev_avg_area and curr_avg_area:\n",
    "        if curr_avg_area > prev_avg_area * FORWARD_THRESHOLD:  # Only 10% increase to detect forward\n",
    "            movement = \"Moving Forward\"\n",
    "\n",
    "    # Strengthen \"Unknown\" detection\n",
    "    if movement in [\"Moving Left\", \"Moving Right\", \"Moving Forward\"]:\n",
    "        if avg_curr_center is None or avg_prev_center is None or abs(avg_curr_center - avg_prev_center) < UNKNOWN_THRESHOLD:\n",
    "            movement = \"Unknown\"\n",
    "\n",
    "    # Distance estimation\n",
    "    car_summary = {\n",
    "        \"Left\": {\"Close\": sum(1 for _, area in curr_data[\"left\"] if area > 50000),\n",
    "                 \"Far\": sum(1 for _, area in curr_data[\"left\"] if area <= 50000)},\n",
    "        \"Right\": {\"Close\": sum(1 for _, area in curr_data[\"right\"] if area > 50000),\n",
    "                  \"Far\": sum(1 for _, area in curr_data[\"right\"] if area <= 50000)},\n",
    "        \"Front\": {\"Close\": sum(1 for _, area in curr_data[\"front\"] if area > 50000),\n",
    "                  \"Far\": sum(1 for _, area in curr_data[\"front\"] if area <= 50000)}\n",
    "    }\n",
    "\n",
    "    return movement, car_summary\n",
    "\n",
    "# Process selected frames\n",
    "prev_frame_data = None\n",
    "movement_summary = []\n",
    "\n",
    "for idx in frame_indices:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read frame {idx}\")\n",
    "        continue\n",
    "\n",
    "    curr_frame_data = detect_cars(frame, idx)\n",
    "\n",
    "    if prev_frame_data is not None:\n",
    "        movement, car_info = analyze_camera_movement(prev_frame_data, curr_frame_data)\n",
    "        movement_summary.append((idx, movement, car_info))\n",
    "\n",
    "    prev_frame_data = curr_frame_data  # Update previous frame data\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print movement analysis\n",
    "for frame, movement, cars in movement_summary:\n",
    "    print(f\"Frame {frame}: {movement} | Cars -> Left: {cars['Left']}, Right: {cars['Right']}, Front: {cars['Front']}\")\n",
    "\n",
    "print(\"✅ Detection complete! Processed frames displayed and saved.\")\n",
    "\n",
    "\"\"\"\n",
    "OLD TECH\n",
    " import torch\n",
    "import cv2\n",
    "import easyocr\n",
    "from ultralytics import YOLO\n",
    "from collections import Counter\n",
    "\n",
    "# Use a better model like \"yolov8m.pt\" or \"yolov8l.pt\" for more accuracy\n",
    "sign_model = YOLO(\"yolov8l.pt\")  # 🔥 Using a larger YOLO model for better sign detection\n",
    "\n",
    "# 🚦 Define Traffic Sign Classes (Custom Dataset Needed for More Accuracy)\n",
    "TRAFFIC_SIGN_CLASSES = [\"stop sign\", \"speed limit sign\", \"parking sign\", \"warning sign\",\n",
    "                         \"one way sign\", \"exit sign\", \"airport sign\", \"construction sign\", \"yield sign\"]\n",
    "\n",
    "# 📖 OCR Model (For extracting text from traffic signs)\n",
    "ocr_reader = easyocr.Reader(['en'])\n",
    "\n",
    "def detect_traffic_signs(video_path):\n",
    "    \"\"\"\n",
    "    Detects traffic signs and extracts text from them.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    detected_signs = Counter()\n",
    "    sign_texts = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = sign_model(frame)  # 🔥 Run Traffic Sign Model\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                class_id = int(box.cls[0].item())\n",
    "                obj_name = sign_model.names[class_id]\n",
    "\n",
    "                if obj_name in TRAFFIC_SIGN_CLASSES:\n",
    "                    detected_signs[obj_name] += 1\n",
    "                    \n",
    "                    # 🔍 Extract text from sign\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].int().tolist()\n",
    "                    sign_region = frame[y1:y2, x1:x2]  # Crop sign\n",
    "                    sign_text = ocr_reader.readtext(sign_region, detail=0)  # Read text\n",
    "\n",
    "                    if sign_text:\n",
    "                        sign_texts.append((obj_name, \" \".join(sign_text)))  # Save sign + text\n",
    "\n",
    "    cap.release()\n",
    "    return detected_signs, sign_texts\n",
    "\n",
    "# 🚀 Example Usage\n",
    "VIDEO1 = VIDEO1\n",
    "\n",
    "detected_signs, sign_texts = detect_traffic_signs(VIDEO1)\n",
    "\n",
    "# 📊 Print Results\n",
    "print(f\"🛑 Traffic Signs Detected: {dict(detected_signs)}\")\n",
    "print(f\"📖 Extracted Text from Signs: {sign_texts}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c625f",
   "metadata": {},
   "source": [
    "2. Fine-Tuning BLIP-2 for Video Question Answering\n",
    "Objective: Teach BLIP-2 to process ego-centric video frames and answer driving-related questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
