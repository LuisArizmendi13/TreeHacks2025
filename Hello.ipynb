{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5dba6-7f9d-4555-8854-3b4a7d910479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d807bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers torch torchvision openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc929dd9",
   "metadata": {},
   "source": [
    "How It Works\n",
    "1. Extracts 8 frames from a 5-second video.\n",
    "2. Encodes frames using BLIP-2â€™s Vision Transformer (ViT).\n",
    "3. Aggregates frame embeddings (mean pooling).\n",
    "4. Sends embeddings + question to ChatGPT.\n",
    "5. ChatGPT generates an answer based on the video context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df11df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import openai  # For ChatGPT API calls\n",
    "\n",
    "# OpenAI API Key (replace with your own key)\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# Load BLIP-2 Processor & Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")  # Or use 'blip2-flan-t5-xl' for T5-based\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\").to(device)\n",
    "\n",
    "# Image Preprocessing Pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_video_frames(video_path, num_frames=8):\n",
    "    \"\"\"\n",
    "    Extracts evenly spaced frames from a video file.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = torch.linspace(0, total_frames - 1, num_frames).long().tolist()\n",
    "\n",
    "    frames = []\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = Image.fromarray(frame)\n",
    "            frames.append(transform(frame))\n",
    "    cap.release()\n",
    "\n",
    "    return torch.stack(frames).to(device)  # Shape: (num_frames, 3, 224, 224)\n",
    "\n",
    "def get_video_embedding(video_path):\n",
    "    \"\"\"\n",
    "    Extracts a single video embedding by processing multiple frames with BLIP-2's ViT.\n",
    "    \"\"\"\n",
    "    frames = extract_video_frames(video_path)\n",
    "    inputs = processor(images=frames, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        video_embeds = model.vision_model(**inputs).last_hidden_state  # Extract ViT embeddings\n",
    "        video_embeds = video_embeds.mean(dim=1)  # Mean pooling across frames\n",
    "    return video_embeds  # Shape: (num_frames, hidden_dim)\n",
    "\n",
    "def generate_chatgpt_response(question, video_embeds):\n",
    "    \"\"\"\n",
    "    Uses video embeddings as context and queries ChatGPT.\n",
    "    \"\"\"\n",
    "    video_context = video_embeds.cpu().numpy().tolist()  # Convert tensor to list for API\n",
    "    prompt = f\"Answer this question based on the given video embeddings:\\n\\n{question}\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI that answers video-related questions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Video embedding: {video_context}\"}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Example Usage\n",
    "video_path = \"sample_video.mp4\"\n",
    "question = \"What is happening in this video?\"\n",
    "\n",
    "video_embeds = get_video_embedding(video_path)\n",
    "answer = generate_chatgpt_response(question, video_embeds)\n",
    "\n",
    "print(f\"Q: {question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c625f",
   "metadata": {},
   "source": [
    "2. Fine-Tuning BLIP-2 for Video Question Answering\n",
    "Objective: Teach BLIP-2 to process ego-centric video frames and answer driving-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df1ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"video_id\": \"example_video.mp4\",\n",
    "  \"frames\": [\"frame_1.jpg\", \"frame_2.jpg\", ..., \"frame_8.jpg\"],  \n",
    "  \"question\": \"Is the car allowed to turn right at this intersection?\",  \n",
    "  \"answer\": \"No, because there is a no-right-turn sign.\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e318349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, TrainingArguments, Trainer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load BLIP-2 Model & Processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = Blip2Processor.from_pretrained(model_name)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Load Your Dataset\n",
    "class DrivingVideoQADataset(Dataset):\n",
    "    def __init__(self, json_file, processor):\n",
    "        with open(json_file, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        frames = [Image.open(frame_path).convert(\"RGB\") for frame_path in sample[\"frames\"]]\n",
    "        inputs = self.processor(images=frames, text=sample[\"question\"], return_tensors=\"pt\", padding=True)\n",
    "        inputs[\"labels\"] = self.processor.tokenizer(sample[\"answer\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}  # Remove batch dim\n",
    "\n",
    "# Initialize Dataset & DataLoader\n",
    "dataset = DrivingVideoQADataset(\"driving_video_qa.json\", processor)\n",
    "\n",
    "# Training Configuration\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./blip2_driving\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Train the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
