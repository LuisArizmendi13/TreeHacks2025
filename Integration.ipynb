{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'COW' from 'my_vars' (/home/ubuntu/TreeHacks2025/my_vars.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minference_sdk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InferenceHTTPClient\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmy_vars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ROBO, COW\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Roboflow API Setup\u001b[39;00m\n\u001b[1;32m      6\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m ROBO  \u001b[38;5;66;03m# Load API Key securely\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'COW' from 'my_vars' (/home/ubuntu/TreeHacks2025/my_vars.py)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from inference_sdk import InferenceHTTPClient\n",
    "from my_vars import ROBO\n",
    "\n",
    "# Roboflow API Setup\n",
    "API_KEY = ROBO  # Load API Key securely\n",
    "MODEL_ID = \"lisa-bjgh5/2\"  # Your YOLOv8 LISA Model ID\n",
    "CLIENT = InferenceHTTPClient(api_url=\"https://detect.roboflow.com\", api_key=API_KEY)\n",
    "\n",
    "def detect_objects_in_frame(frame):\n",
    "    \"\"\"Detect objects in a single frame using the API.\"\"\"\n",
    "    try:\n",
    "        result = CLIENT.infer(frame, model_id=MODEL_ID)  # Send frame directly, no need to save it as an image\n",
    "        frame_detections = []\n",
    "\n",
    "        for pred in result.get(\"predictions\", []):\n",
    "            frame_detections.append({\n",
    "                \"label\": pred[\"class\"],\n",
    "                \"confidence\": pred[\"confidence\"],\n",
    "                \"bbox\": {\n",
    "                    \"x\": int(pred[\"x\"]),\n",
    "                    \"y\": int(pred[\"y\"]),\n",
    "                    \"width\": int(pred[\"width\"]),\n",
    "                    \"height\": int(pred[\"height\"])\n",
    "                }\n",
    "            })\n",
    "\n",
    "        return frame_detections  # List of detected objects\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during object detection: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def process_video_frames(video_path):\n",
    "    \"\"\"Extracts three key frames from a video and runs object detection.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = [0, total_frames // 2, total_frames - 1]  # Beginning, middle, end\n",
    "    detections = []\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error: Could not read frame {frame_idx}\")\n",
    "            continue\n",
    "\n",
    "        frame_detections = detect_objects_in_frame(frame, frame_idx)\n",
    "        if frame_detections:\n",
    "            detections.append(frame_detections)\n",
    "\n",
    "        time.sleep(0.2)  # Avoid API rate limits\n",
    "\n",
    "    cap.release()\n",
    "    return detections\n",
    "\n",
    "# Example usage\n",
    "#detections = process_video_frames(VIDEO1)\n",
    "#print(detections)\n",
    "\n",
    "print(\"✅ Inference complete! Detection results returned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 416x640 4 cars, 3.4ms\n",
      "Speed: 1.8ms preprocess, 3.4ms inference, 0.8ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 293\u001b[0m\n\u001b[1;32m    290\u001b[0m             questions\u001b[38;5;241m.\u001b[39mappend(row[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Assuming questions are in the second column\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Run sequential processing\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m video_results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m save_results_to_csv(video_results)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# Show the file path\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[47], line 222\u001b[0m, in \u001b[0;36mprocess_inputs\u001b[0;34m(video_list, question_list)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;66;03m# Step 1: Extract Frames (All models will use these)\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m frames, movement_data, detections \u001b[38;5;241m=\u001b[39m \u001b[43mextract_video_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Step 2: Batch Process CLIP and BLIP-2\u001b[39;00m\n\u001b[1;32m    225\u001b[0m video_descriptions \u001b[38;5;241m=\u001b[39m get_video_descriptions(frames)  \u001b[38;5;66;03m# BLIP-2 runs in batch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[47], line 54\u001b[0m, in \u001b[0;36mextract_video_frames\u001b[0;34m(video_path, num_frames)\u001b[0m\n\u001b[1;32m     51\u001b[0m pil_frame \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB))\n\u001b[1;32m     52\u001b[0m frames\u001b[38;5;241m.\u001b[39mappend(pil_frame)\n\u001b[0;32m---> 54\u001b[0m frame_detections \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_objects_in_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_detections:\n\u001b[1;32m     56\u001b[0m     detections\u001b[38;5;241m.\u001b[39mappend(frame_detections)\n",
      "Cell \u001b[0;32mIn[39], line 13\u001b[0m, in \u001b[0;36mdetect_objects_in_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect objects in a single frame using the API.\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mCLIENT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Send frame directly, no need to save it as an image\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     frame_detections \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m, []):\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/site-packages/inference_sdk/http/client.py:94\u001b[0m, in \u001b[0;36mwrap_errors.<locals>.decorate\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/site-packages/inference_sdk/http/client.py:400\u001b[0m, in \u001b[0;36mInferenceHTTPClient.infer\u001b[0;34m(self, inference_input, model_id)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run inference on one or more images.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    HTTPClientError: If there is an error with the server connection.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__client_mode \u001b[38;5;129;01mis\u001b[39;00m HTTPClientMode\u001b[38;5;241m.\u001b[39mV0:\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_from_api_v0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_from_api_v1(\n\u001b[1;32m    405\u001b[0m     inference_input\u001b[38;5;241m=\u001b[39minference_input,\n\u001b[1;32m    406\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    407\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/site-packages/inference_sdk/http/client.py:489\u001b[0m, in \u001b[0;36mInferenceHTTPClient.infer_from_api_v0\u001b[0;34m(self, inference_input, model_id)\u001b[0m\n\u001b[1;32m    479\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__inference_configuration\u001b[38;5;241m.\u001b[39mto_legacy_call_parameters())\n\u001b[1;32m    480\u001b[0m requests_data \u001b[38;5;241m=\u001b[39m prepare_requests_data(\n\u001b[1;32m    481\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__api_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id_chunks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_id_chunks[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    482\u001b[0m     encoded_inference_inputs\u001b[38;5;241m=\u001b[39mencoded_inference_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m     image_placement\u001b[38;5;241m=\u001b[39mImagePlacement\u001b[38;5;241m.\u001b[39mDATA,\n\u001b[1;32m    488\u001b[0m )\n\u001b[0;32m--> 489\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_requests_packages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequests_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequests_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRequestMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_concurrent_requests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inference_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_concurrent_requests\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m request_data, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(requests_data, responses):\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/site-packages/inference_sdk/http/utils/executors.py:59\u001b[0m, in \u001b[0;36mexecute_requests_packages\u001b[0;34m(requests_data, request_method, max_concurrent_requests)\u001b[0m\n\u001b[1;32m     57\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m requests_data_package \u001b[38;5;129;01min\u001b[39;00m requests_data_packages:\n\u001b[0;32m---> 59\u001b[0m     responses \u001b[38;5;241m=\u001b[39m \u001b[43mmake_parallel_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequests_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequests_data_package\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     results\u001b[38;5;241m.\u001b[39mextend(responses)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/site-packages/inference_sdk/http/utils/executors.py:85\u001b[0m, in \u001b[0;36mmake_parallel_requests\u001b[0;34m(requests_data, request_method)\u001b[0m\n\u001b[1;32m     83\u001b[0m make_request_closure \u001b[38;5;241m=\u001b[39m partial(make_request, request_method\u001b[38;5;241m=\u001b[39mrequest_method)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mworkers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_request_closure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequests_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/miniconda/envs/treehack/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import cv2\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "import openai\n",
    "from PIL import Image\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# OpenAI API Key\n",
    "MY_KEY = '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'\n",
    "client = openai.OpenAI(api_key=MY_KEY)\n",
    "\n",
    "# Load Models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\").to(device)\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "yolo_model = YOLO(\"yolov8n.pt\")  # Using YOLOv8 for car detection\n",
    "\n",
    "# Thresholds\n",
    "MIN_SHIFT_THRESHOLD = 20  \n",
    "FORWARD_THRESHOLD = 1.1  \n",
    "UNKNOWN_THRESHOLD = 10   \n",
    "\n",
    "def extract_video_frames(video_path, num_frames=8):\n",
    "    \"\"\"Extracts 8 evenly spaced frames from the video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_center = frame_width // 2\n",
    "\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames).astype(int).tolist()\n",
    "    prev_frame_data = None\n",
    "    movement_summary = []\n",
    "    frames = []\n",
    "    detections = []\n",
    "\n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error: Could not read frame {idx}\")\n",
    "            continue\n",
    "\n",
    "        # Convert to PIL format for CLIP & BLIP-2\n",
    "        pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        frames.append(pil_frame)\n",
    "\n",
    "        frame_detections = detect_objects_in_frame(frame)\n",
    "        if frame_detections:\n",
    "            detections.append(frame_detections)\n",
    "        # Run YOLO on this frame\n",
    "        curr_frame_data = detect_cars_in_frame(frame, idx, frame_center)\n",
    "\n",
    "        # Analyze movement between frames\n",
    "        if prev_frame_data is not None:\n",
    "            movement, car_info = analyze_camera_movement(prev_frame_data, curr_frame_data)\n",
    "            movement_summary.append((idx, movement, car_info))\n",
    "\n",
    "        prev_frame_data = curr_frame_data\n",
    "\n",
    "    cap.release()\n",
    "    return frames, movement_summary, detections\n",
    "\n",
    "def get_video_descriptions(frames):\n",
    "    \"\"\"Generates textual descriptions for frames using BLIP-2.\"\"\"\n",
    "    if not frames:\n",
    "        return [\"No description available\"] * len(frames)\n",
    "\n",
    "    inputs = processor(images=frames, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_length=50)\n",
    "        descriptions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "def detect_cars_in_frame(frame, frame_num, frame_center):\n",
    "    \"\"\"Detect cars in a frame and categorize their positions.\"\"\"\n",
    "    results = yolo_model(frame)\n",
    "    car_positions = {\"left\": [], \"right\": [], \"front\": []}\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            label = result.names[int(box.cls[0])]\n",
    "\n",
    "            if label == \"car\":\n",
    "                box_center_x = (x1 + x2) // 2\n",
    "                area = (x2 - x1) * (y2 - y1)  \n",
    "\n",
    "                if box_center_x < frame_center * 0.75:\n",
    "                    car_positions[\"left\"].append((box_center_x, area))\n",
    "                elif box_center_x > frame_center * 1.25:\n",
    "                    car_positions[\"right\"].append((box_center_x, area))\n",
    "                else:\n",
    "                    car_positions[\"front\"].append((box_center_x, area))\n",
    "\n",
    "    return car_positions\n",
    "\n",
    "def analyze_camera_movement(prev_data, curr_data):\n",
    "    \"\"\"Analyzes camera movement using object tracking.\"\"\"\n",
    "    if not prev_data or not curr_data:\n",
    "        return \"Unknown\", {}\n",
    "\n",
    "    movement = \"Unknown\"\n",
    "    # Get average X positions of objects in each region\n",
    "    avg_prev_left = np.mean([x for x, _ in prev_data[\"left\"]]) if prev_data[\"left\"] else None\n",
    "    avg_curr_left = np.mean([x for x, _ in curr_data[\"left\"]]) if curr_data[\"left\"] else None\n",
    "\n",
    "    avg_prev_right = np.mean([x for x, _ in prev_data[\"right\"]]) if prev_data[\"right\"] else None\n",
    "    avg_curr_right = np.mean([x for x, _ in curr_data[\"right\"]]) if curr_data[\"right\"] else None\n",
    "\n",
    "    avg_prev_center = np.mean([x for x, _ in prev_data[\"front\"]]) if prev_data[\"front\"] else None\n",
    "    avg_curr_center = np.mean([x for x, _ in curr_data[\"front\"]]) if curr_data[\"front\"] else None\n",
    "\n",
    "    # Left/right movement detection\n",
    "    if avg_prev_left and avg_curr_left and avg_curr_left > avg_prev_left + MIN_SHIFT_THRESHOLD:\n",
    "        movement = \"Moving Right\"\n",
    "    elif avg_prev_right and avg_curr_right and avg_curr_right < avg_prev_right - MIN_SHIFT_THRESHOLD:\n",
    "        movement = \"Moving Left\"\n",
    "\n",
    "    # Forward detection (More lenient)\n",
    "    prev_avg_area = np.mean([area for _, area in prev_data[\"front\"]]) if prev_data[\"front\"] else None\n",
    "    curr_avg_area = np.mean([area for _, area in curr_data[\"front\"]]) if curr_data[\"front\"] else None\n",
    "\n",
    "    if prev_avg_area and curr_avg_area:\n",
    "        if curr_avg_area > prev_avg_area * FORWARD_THRESHOLD:  # Only 10% increase to detect forward\n",
    "            movement = \"Moving Forward\"\n",
    "\n",
    "    # Strengthen \"Unknown\" detection\n",
    "    if movement in [\"Moving Left\", \"Moving Right\", \"Moving Forward\"]:\n",
    "        if avg_curr_center is None or avg_prev_center is None or abs(avg_curr_center - avg_prev_center) < UNKNOWN_THRESHOLD:\n",
    "            movement = \"Unknown\"\n",
    "\n",
    "    # Distance estimation\n",
    "    car_summary = {\n",
    "        \"Left\": {\"Close\": sum(1 for _, area in curr_data[\"left\"] if area > 50000),\n",
    "                 \"Far\": sum(1 for _, area in curr_data[\"left\"] if area <= 50000)},\n",
    "        \"Right\": {\"Close\": sum(1 for _, area in curr_data[\"right\"] if area > 50000),\n",
    "                  \"Far\": sum(1 for _, area in curr_data[\"right\"] if area <= 50000)},\n",
    "        \"Front\": {\"Close\": sum(1 for _, area in curr_data[\"front\"] if area > 50000),\n",
    "                  \"Far\": sum(1 for _, area in curr_data[\"front\"] if area <= 50000)}\n",
    "    }\n",
    "\n",
    "    return movement, car_summary\n",
    "\n",
    "\n",
    "def generate_chatgpt_responses(questions, video_summaries):\n",
    "    \"\"\"\n",
    "    Uses GPT-4 Turbo to generate responses in parallel.\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "        f\"\"\"\n",
    "        You will receive information from 8 frames of a 5-second video. The details include:\n",
    "\n",
    "        - **Description (BLIP-2):** A broader summary of the video’s contents.\n",
    "        - **LISA Detections:** Any detected street signs, along with confidence scores and bounding boxes. \n",
    "        - A \"None\" detection does **not** mean no signs exist.\n",
    "        - LISA may confuse similar-looking signs at low confidence.\n",
    "        - **Camera Movement:** Directional movement of the vehicle (ego motion). This can be unreliable.\n",
    "        - **Car Summary:** The number of cars on the left, right, and front, and whether they are close or far.\n",
    "\n",
    "        Given this information, provide **only the letter of your best answer** to the following question with no other text: \n",
    "        {question}\n",
    "\n",
    "        **Video Summary:**\n",
    "        {summary}\n",
    "        \"\"\"\n",
    "        for question, summary in zip(questions, video_summaries)\n",
    "    ]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        responses = list(executor.map(chatgpt_request, prompts))\n",
    "\n",
    "    return responses\n",
    "\n",
    "def chatgpt_request(prompt):\n",
    "    \"\"\"\n",
    "    Sends a request to GPT-4o, but falls back to GPT-3.5-Turbo if unavailable.\n",
    "    \"\"\"\n",
    "    models = [\"gpt-4o\", \"gpt-4o-mini\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-0125\"]  # Try GPT-4o first, fallback to GPT-3.5-Turbo\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful AI that analyzes driving videos for a self-driving car.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=100\n",
    "            )\n",
    "            return response.choices[0].message.content  # Return response if successful\n",
    "        \n",
    "        except openai.OpenAIError as e:\n",
    "            print(f\"⚠️ {model} unavailable, trying next model... (Error: {e})\")\n",
    "            continue  # Try the next model in the list\n",
    "\n",
    "    return \"❌ Error: Both GPT-4o and GPT-3.5-Turbo are unavailable.\"\n",
    "\n",
    "\n",
    "def process_inputs(video_list, question_list):\n",
    "    \"\"\"Processes videos sequentially while ensuring synchronization between YOLO (one-by-one) and batch processing (BLIP-2 & CLIP).\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(len(video_list)):\n",
    "        video_path = video_list[i]\n",
    "        question = question_list[i]\n",
    "\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Skipping {video_path}: File not found\")\n",
    "            continue\n",
    "\n",
    "        # Step 1: Extract Frames (All models will use these)\n",
    "        frames, movement_data, detections = extract_video_frames(video_path)\n",
    "\n",
    "        # Step 2: Batch Process CLIP and BLIP-2\n",
    "        video_descriptions = get_video_descriptions(frames)  # BLIP-2 runs in batch\n",
    "\n",
    "        # Step 3: Process YOLO One-by-One (Per Frame)\n",
    "        frame_descriptions = []\n",
    "        for idx, frame in enumerate(frames):\n",
    "            _, movement, car_summary = movement_data[idx] if idx < len(movement_data) else (idx, \"Unknown\", {})\n",
    "            frame_detections = detections[idx] if idx < len(detections) else []\n",
    "\n",
    "            # Ensure YOLO results align with frame order\n",
    "            frame_info = (\n",
    "                f\"Frame {idx+1}: \\n\"\n",
    "                f\" - Description (BLIP-2): {video_descriptions[idx]}\\n\"\n",
    "                f\" - LISA Detections: {', '.join([str(detection) for detection in frame_detections]) if frame_detections else 'None'}\\n\"\n",
    "                f\" - Camera Movement: {movement}\\n\"\n",
    "                f\" - Car Summary: {car_summary}\\n\"\n",
    "            )\n",
    "            frame_descriptions.append(frame_info)\n",
    "\n",
    "        # Step 4: Format All Frame Data for ChatGPT\n",
    "        formatted_description = \"\\n\".join(frame_descriptions)\n",
    "        answer = generate_chatgpt_responses([question], [formatted_description])[0]\n",
    "        print(answer)\n",
    "        answer = answer[0] if len(answer) >= 2 else answer\n",
    "        if (answer != 'A') and (answer != 'B') and (answer != 'C') and (answer != 'D'):\n",
    "            answer = 'C'\n",
    "        print(answer)\n",
    "        video_id = os.path.basename(video_path).split(\"_\")[-1].split(\".\")[0]\n",
    "        results.append((video_id, answer))\n",
    "        print(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results_to_csv(results, output_csv=\"/mnt/data/output.csv\"):\n",
    "    \"\"\"\n",
    "    Saves processed video results to a CSV file using Pandas.\n",
    "\n",
    "    :param results: List of tuples (video_id, answer).\n",
    "    :param output_csv: Path for the output CSV file.\n",
    "    :return: File path of the saved CSV file.\n",
    "    \"\"\"\n",
    "    # Convert results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(results, columns=[\"id\", \"answer\"])\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(output_csv, index=False)  # `index=False` prevents adding an extra index column\n",
    "\n",
    "    print(f\"Results saved to {output_csv}\")\n",
    "    return output_csv  # Return the file path\n",
    "\n",
    "# Load Videos\n",
    "video_directory = \"/home/ubuntu/TreeHacks2025/data/videos/videos\"\n",
    "video_files = sorted(\n",
    "    [os.path.join(video_directory, f) for f in os.listdir(video_directory) if f.endswith(\".mp4\")]\n",
    ")\n",
    "\n",
    "# Load Questions from CSV\n",
    "question_file = \"/home/ubuntu/TreeHacks2025/data/questions.csv\"\n",
    "questions = []\n",
    "\n",
    "with open(question_file, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip header row if it exists\n",
    "    for row in reader:\n",
    "        if row:  # Ensure row is not empty\n",
    "            questions.append(row[1])  # Assuming questions are in the second column\n",
    "\n",
    "# Run sequential processing\n",
    "video_results = process_inputs(video_files, questions)\n",
    "\n",
    "csv_file_path = save_results_to_csv(video_results)\n",
    "\n",
    "# Show the file path\n",
    "csv_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treehack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
